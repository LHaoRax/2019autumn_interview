### MLE(最大似然估计)、MAP(最大后验概率估计)和贝叶斯估计的区别与联系

贝叶斯公式：
$$
p(X|Y)=\frac{p(Y|X)\cdot p(X)}{p(Y)}
$$
似然（条件概率）：
$$
p(Y|X)
$$
先验概率：
$$
P(X)
$$
> 先验概率是观察到数据Y之前，根据经验得到的X假说的几率

后验概率：
$$
P(X|Y)
$$

> 后验概率是给定证据Y之后，假说X的概率
>

证据因子：
$$
P(Y)
$$

> Y代表证据，证据对应新的数据，没有用来计算先验概率的数据

贝叶斯公式也称为逆概率公式，将后验概率转化为基于似然函数和先验概率的计算表达式即：
$$
posterior=\frac{likelihood\cdot prior}{evidence}
$$

#### MLE(最大似然估计)

最大似然估计就是要用**似然函数取到最大值**时的参数值作为**估计值**，似然函数可以写做：
$$
L(X|Y)=p(Y|X)=\prod_{y\in Y}p(Y=y|X)
$$
由于有连乘运算，通常对数似然函数取对数计算简便，即对数似然函数。最大似然估计问题可以写做：
$$
\hat{\theta_{ML}}=argmay_{X}L(X|Y)=argmay_{X}\Sigma_{y\in Y}logp(y|X)
$$


#### MAP(最大后验概率估计)

最大后验概率与最大似然估计相似，不同点在于估计`X`的函数中允许加入一个先验概率`p(X)`，也就是说此时不是要求似然函数最大，而是要贝叶斯公式计算出的整个后验概率最大，即
$$
\begin{split}
\hat{\theta_{ML}}&=argmay_{X}\frac{p(Y|X)\cdot p(X)}{p(Y)}\\
&=argmay_{X}p(Y|X)\cdot p(X)\\
&=argmay_{X}\{L(X|Y)+logp(X)\}\\
&=argmay_{X}\{\Sigma_{y\in Y}logp(y|X)+logp(X)\}
\end{split}
$$
注意这里**P(Y)与参数X无关**，因此等价于要使分子最大。与最大似然估计相比，现在需要**多加上一个先验分布概率的对数**。在实际应用中，这个先验可以用来描述人们已经知道或者接受的普遍规律。例如在扔硬币的试验中，每次抛出正面发生的概率应该服从一个概率分布，这个概率在0.5处取得最大值，这个分布就是先验分布。

和最大似然估计的结果对比可以发现结果中多了先验的作用。并且先验作用越大，为了改变先验分布传递的belief所需要的观察值就越多。

#### 贝叶斯估计

贝叶斯估计是最大后验估计的进一步扩展，和MAP一样，认为参数取值不固定，而是服从一个先验分布。但MAP是**直接估计出参数的值**，而贝叶斯估计是**估计出参数的分布**。现在我们要求的就不再是后验概率，而是$p(Y)$，既观察到evidence的概率。当新的数据被观察到时，后验概率可以自动随之调整。根据全概率公式：
$$
p(Y)=\int_{x\in X}p(Y|x)p(x)dx
$$

那么如何用贝叶斯估计来做预测呢？如果我们想求一个新值$\hat{y}$概率，可以由

$$
\begin{split}
p(\hat{y}|Y)&=\int_{x\in X}p(\hat{y}|x)p(x|Y)dx\\
&=\int_{x\in X}p(\hat{y}|x)\frac{p(Y|x)\cdot p(x|\alpha)}{p(Y)}dx
\end{split}
$$

上述公式是贝叶斯估计的核心，它把**类条件概率密度$p(\hat{y}|x)$**和**未知参数的后验概率密度$p(x|Y)$**联系起来，就是贝叶斯估计使用**贝叶斯定理去估计参数的后验概率密度**

> 没有太明白贝叶斯估计

#### 三者的区别与联系

##### 联系

+ 三者均从贝叶斯公式出发
+ MAP和MLE均估计的是参数的点估计，
+ 如果不知道关于假设的任何概率，任何的假设拥有相同的概率，两者相同
+ 如果数据量足够大，两者趋向一致

##### 区别

+ MAP相比于MLE加入了先验概率，MLE只考虑了观察序列的概率
+ MLE考虑了所有训练数据拟合程度，没有考虑先验知识，把错误点加入模型中，容易导致过拟合
+ 如果独立条件满足，MAP与朴素贝叶斯相

